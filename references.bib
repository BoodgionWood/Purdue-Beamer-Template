@article{CohenMichaelB2016ISTL,
title = {Input Sparsity Time Low-Rank Approximation via Ridge Leverage Score Sampling},
year = {2016},
abstract = {We present a new algorithm for finding a near optimal low-rank approximation of a matrix \(A\) in \(O(nnz(A))\) time. Our method is based on a recursive sampling scheme for computing a representative subset of \(A\)'s columns, which is then used to find a low-rank approximation. This approach differs substantially from prior \(O(nnz(A))\) time algorithms, which are all based on fast Johnson-Lindenstrauss random projections. It matches the guarantees of these methods while offering a number of advantages. Not only are sampling algorithms faster for sparse and structured data, but they can also be applied in settings where random projections cannot. For example, we give new single-pass streaming algorithms for the column subset selection and projection-cost preserving sample problems. Our method has also been used to give the fastest algorithms for provably approximating kernel matrices [MM16].},
author = {Cohen, Michael B and Musco, Cameron and Musco, Christopher},
address = {Ithaca},
copyright = {2016. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
issn = {2331-8422},
journal = {arXiv.org},
keywords = {Algorithms ; Sampling ; Mathematical analysis},
language = {eng},
publisher = {Cornell University Library, arXiv.org},
}

@inproceedings{macqueen1967some,
  title={Some methods for classification and analysis of multivariate observations},
  author={MacQueen, James and others},
  booktitle={Proceedings of the fifth Berkeley symposium on mathematical statistics and probability},
  volume={1},
  number={14},
  pages={281--297},
  year={1967},
  organization={Oakland, CA, USA}
}

@article{borodachov2014low,
  title={Low complexity methods for discretizing manifolds via Riesz energy minimization},
  author={Borodachov, Sergiy V and Hardin, Douglas P and Saff, Edward B},
  journal={Foundations of Computational Mathematics},
  volume={14},
  pages={1173--1208},
  year={2014},
  publisher={Springer}
}

@article{joseph2015sequential,
  title={Sequential exploration of complex surfaces using minimum energy designs},
  author={Joseph, V Roshan and Dasgupta, Tirthankar and Tuo, Rui and Wu, CF Jeff},
  journal={Technometrics},
  volume={57},
  number={1},
  pages={64--74},
  year={2015},
  publisher={Taylor \& Francis}
}

@book{fang1993number,
  title={Number-theoretic methods in statistics},
  author={Fang, Kai-Tai and Wang, Yuan},
  volume={51},
  year={1993},
  publisher={CRC Press}
}

@book{graf2007foundations,
  title={Foundations of quantization for probability distributions},
  author={Graf, Siegfried and Luschgy, Harald},
  year={2007},
  publisher={Springer}
}


@article{LucicMario2018TGMM,
title = {Training Gaussian Mixture Models at Scale via Coresets},
year = {2018},
abstract = {How can we train a statistical mixture model on a massive data set? In this work we show how to construct coresets for mixtures of Gaussians. A coreset is a weighted subset of the data, which guarantees that models fitting the coreset also provide a good fit for the original data set. We show that, perhaps surprisingly, Gaussian mixtures admit coresets of size polynomial in dimension and the number of mixture components, while being independent of the data set size. Hence, one can harness computationally intensive algorithms to compute a good approximation on a significantly smaller data set. More importantly, such coresets can be efficiently constructed both in distributed and streaming settings and do not impose restrictions on the data generating process. Our results rely on a novel reduction of statistical estimation to problems in computational geometry and new combinatorial complexity results for mixtures of Gaussians. Empirical evaluation on several real-world datasets suggests that our coreset-based approach enables significant reduction in training-time with negligible approximation error.},
author = {Lucic, Mario and Faulkner, Matthew and Krause, Andreas and Feldman, Dan},
address = {Ithaca},
copyright = {2018. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
issn = {2331-8422},
journal = {arXiv.org},
keywords = {Training ; Algorithms ; Mathematical analysis ; Set theory ; Models, Statistical ; Polynomials ; Combinatorial analysis},
language = {eng},
publisher = {Cornell University Library, arXiv.org},
}


@article{CourtyNicolas2017OTfD,
title = {Optimal Transport for Domain Adaptation},
volume = {39},
year = {2017},
abstract = {Domain adaptation is one of the most challenging tasks of modern data analytics. If the adaptation is done correctly, models built on a specific data representation become more robust when confronted to data depicting the same classes, but described by another observation system. Among the many strategies proposed, finding domain-invariant representations has shown excellent properties, in particular since it allows to train a unique classifier effective in all domains. In this paper, we propose a regularized unsupervised optimal transportation model to perform the alignment of the representations in the source and target domains. We learn a transportation plan matching both PDFs, which constrains labeled samples of the same class in the source domain to remain close during transport. This way, we exploit at the same time the labeled samples in the source and the distributions observed in both domains. Experiments on toy and challenging real visual adaptation examples show the interest of the method, that consistently outperforms state of the art approaches. In addition, numerical experiments show that our approach leads to better performances on domain invariant deep learning features and can be easily adapted to the semi-supervised case where few labeled samples are available in the target domain.},
author = {Courty, Nicolas and Flamary, Remi and Tuia, Devis and Rakotomamonjy, Alain},
address = {United States},
copyright = {Distributed under a Creative Commons Attribution 4.0 International License},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Training ; Data Analysis ; Transportation ; Distribution (Probability theory) ; Classification ; Machine learning ; Invariants ; Adaptation ; Statistics ; Computer science},
language = {eng},
number = {9},
pages = {1853-1865},
publisher = {IEEE},
}


@article{ArjovskyMartin2017WG,
title = {Wasserstein GAN},
year = {2017},
abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
address = {Ithaca},
copyright = {2017. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
issn = {2331-8422},
journal = {arXiv.org},
keywords = {Learning curve (Psychometrics) ; Algorithms},
language = {eng},
publisher = {Cornell University Library, arXiv.org},
}


@article{CampbellTrevor2018BCCv,
title = {Bayesian Coreset Construction via Greedy Iterative Geodesic Ascent},
year = {2018},
abstract = {Coherent uncertainty quantification is a key strength of Bayesian methods. But modern algorithms for approximate Bayesian posterior inference often sacrifice accurate posterior uncertainty estimation in the pursuit of scalability. This work shows that previous Bayesian coreset construction algorithms---which build a small, weighted subset of the data that approximates the full dataset---are no exception. We demonstrate that these algorithms scale the coreset log-likelihood suboptimally, resulting in underestimated posterior uncertainty. To address this shortcoming, we develop greedy iterative geodesic ascent (GIGA), a novel algorithm for Bayesian coreset construction that scales the coreset log-likelihood optimally. GIGA provides geometric decay in posterior approximation error as a function of coreset size, and maintains the fast running time of its predecessors. The paper concludes with validation of GIGA on both synthetic and real datasets, demonstrating that it reduces posterior approximation error by orders of magnitude compared with previous coreset constructions.},
author = {Campbell, Trevor and Broderick, Tamara},
address = {Ithaca},
copyright = {2018. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
issn = {2331-8422},
journal = {arXiv.org},
keywords = {Uncertainty ; Algorithms ; Mathematical analysis ; Bayesian statistical decision theory},
language = {eng},
publisher = {Cornell University Library, arXiv.org},
}


@article{ColemanCody2019SvPE,
title = {Selection via Proxy: Efficient Data Selection for Deep Learning},
year = {2019},
abstract = {Data selection methods, such as active learning and core-set selection, are useful tools for machine learning on large datasets. However, they can be prohibitively expensive to apply in deep learning because they depend on feature representations that need to be learned. In this work, we show that we can greatly improve the computational efficiency by using a small proxy model to perform data selection (e.g., selecting data points to label for active learning). By removing hidden layers from the target model, using smaller architectures, and training for fewer epochs, we create proxies that are an order of magnitude faster to train. Although these small proxy models have higher error rates, we find that they empirically provide useful signals for data selection. We evaluate this "selection via proxy" (SVP) approach on several data selection tasks across five datasets: CIFAR10, CIFAR100, ImageNet, Amazon Review Polarity, and Amazon Review Full. For active learning, applying SVP can give an order of magnitude improvement in data selection runtime (i.e., the time it takes to repeatedly train and select points) without significantly increasing the final error (often within 0.1%). For core-set selection on CIFAR10, proxies that are over 10x faster to train than their larger, more accurate targets can remove up to 50% of the data without harming the final accuracy of the target, leading to a 1.6x end-to-end training time improvement.},
author = {Coleman, Cody and Yeh, Christopher and Mussmann, Stephen and Mirzasoleiman, Baharan and Bailis, Peter and Liang, Percy and Leskovec, Jure and Zaharia, Matei},
address = {Ithaca},
copyright = {2020. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
issn = {2331-8422},
journal = {arXiv.org},
keywords = {Training ; Active learning ; Machine learning ; Artificial intelligence},
language = {eng},
publisher = {Cornell University Library, arXiv.org},
}


@article{FeldmanDan2020CAus,
title = {Coresets: An updated survey},
volume = {10},
year = {2020},
abstract = {In optimization or machine learning problems we are given a set of items, usually points in some metric space, and the goal is to minimize or maximize an objective function over some space of candidate solutions. For example, in clustering problems, the input is a set of points in some metric space, and a common goal is to compute a set of centers in some other space (points, lines) that will minimize the sum of distances to these points. In database queries, we may need to compute such a some for a specific query set of k centers. However, traditional algorithms cannot handle modern systems that require parallel real‐time computations of infinite distributed streams from sensors such as GPS, audio or video that arrive to a cloud, or networks of weaker devices such as smartphones or robots. Core‐set is a “small data” summarization of the input “big data,” where every possible query has approximately the same answer on both data sets. Generic techniques enable efficient coreset maintenance of streaming, distributed and dynamic data. Traditional algorithms can then be applied on these coresets to maintain the approximated optimal solutions. The challenge is to design coresets with provable tradeoff between their size and approximation error. This survey summarizes such constructions in a retrospective way, that aims to unified and simplify the state‐of‐the‐art.
This article is categorized under
Algorithmic Development > Structure Discovery
Fundamental Concepts of Data and Knowledge > Big Data Mining
Technologies > Machine Learning
Algorithmic Development > Scalable Statistical Methods
Solving a problem on a (small) data summarization (core‐set) yields provably good approximation to the optimal solution of the original (big) data.},
author = {Feldman, Dan},
address = {Hoboken, USA},
copyright = {2019 Wiley Periodicals, Inc.},
issn = {1942-4787},
journal = {Wiley interdisciplinary reviews. Data mining and knowledge discovery},
keywords = {Big data ; Cloud computing ; Algorithms ; Electronic apparatus and appliances ; Data Management ; Machine learning ; Statistical methods ; Statistics ; Cluster analysis ; Computer science},
language = {eng},
number = {1},
pages = {e1335-n/a},
publisher = {Wiley Periodicals, Inc},
}


@article{LeiShiye2024ACSo,
title = {A Comprehensive Survey of Dataset Distillation},
volume = {46},
year = {2024},
abstract = {Deep learning technology has developed unprecedentedly in the last decade and has become the primary choice in many application domains. This progress is mainly attributed to a systematic collaboration in which rapidly growing computing resources encourage advanced algorithms to deal with massive data. However, it has gradually become challenging to handle the unlimited growth of data with limited computing power. To this end, diverse approaches are proposed to improve data processing efficiency. Dataset distillation, a dataset reduction method, addresses this problem by synthesizing a small typical dataset from substantial data and has attracted much attention from the deep learning community. Existing dataset distillation methods can be taxonomized into meta-learning and data matching frameworks according to whether they explicitly mimic the performance of target data. Although dataset distillation has shown surprising performance in compressing datasets, there are still several limitations such as distilling high-resolution data or data with complex label spaces. This paper provides a holistic understanding of dataset distillation from multiple aspects, including distillation frameworks and algorithms, factorized dataset distillation, performance comparison, and applications. Finally, we discuss challenges and promising directions to further promote future studies on dataset distillation.},
author = {Lei, Shiye and Tao, Dacheng},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Training ; Surveys ; Ciphers},
language = {eng},
number = {1},
pages = {17-32},
publisher = {IEEE},
}


@inproceedings{SagirogluSeref2013BdAr,
title = {Big data: A review},
year = {2013},
abstract = {Big data is a term for massive data sets having large, more varied and complex structure with the difficulties of storing, analyzing and visualizing for further processes or results. The process of research into massive amounts of data to reveal hidden patterns and secret correlations named as big data analytics. These useful informations for companies or organizations with the help of gaining richer and deeper insights and getting an advantage over the competition. For this reason, big data implementations need to be analyzed and executed as accurately as possible. This paper presents an overview of big data's content, scope, samples, methods, advantages and challenges and discusses privacy concern on it.},
author = {Sagiroglu, Seref and Sinanc, Duygu},
booktitle = {2013 International Conference on Collaboration Technologies and Systems (CTS)},
isbn = {1467364037},
keywords = {Associations, institutions, etc. ; Information resources management ; Big data ; Value},
language = {eng},
pages = {42-47},
publisher = {IEEE},
}


@article{MakSimon2018SP,
title = {SUPPORT POINTS},
volume = {46},
year = {2018},
abstract = {This paper introduces a new way to compact a continuous probability distribution F into a set of representative points called support points. These points are obtained by minimizing the energy distance, a statistical potential measure initially proposed by Székely and Rizzo [InterStat 5 (2004) 1–6] for testing goodness-of-fit. The energy distance has two appealing features. First, its distance-based structure allows us to exploit the duality between powers of the Euclidean distance and its Fourier transform for theoretical analysis. Using this duality, we show that support points converge in distribution to F, and enjoy an improved error rate to Monte Carlo for integrating a large class of functions. Second, the minimization of the energy distance can be formulated as a difference-of-convex program, which we manipulate using two algorithms to efficiently generate representative point sets. In simulation studies, support points provide improved integration performance to both Monte Carlo and a specific quasi-Monte Carlo method. Two important applications of support points are then highlighted: (a) as a way to quantify the propagation of uncertainty in expensive simulations and (b) as a method to optimally compact Markov chain Monte Carlo (MCMC) samples in Bayesian computation.},
author = {Mak, Simon and Joseph, V. Roshan},
address = {Hayward},
copyright = {Institute of Mathematical Statistics, 2018},
issn = {0090-5364},
journal = {The Annals of statistics},
keywords = {Statistics ; Computer simulation ; Energy conservation ; Markov processes ; Statistical methods ; Bayesian statistical decision theory ; Monte Carlo method},
language = {eng},
number = {6A},
pages = {2562-2592},
publisher = {Institute of Mathematical Statistics},
}

@inproceedings{pele2009fast,
  title={Fast and robust earth mover's distances},
  author={Pele, Ofir and Werman, Michael},
  booktitle={2009 IEEE 12th international conference on computer vision},
  pages={460--467},
  year={2009},
  organization={IEEE}
}

@article{cuturi2018semidual,
  title={Semi-dual regularized optimal transport},
  author={Cuturi, Marco and Peyr{\'e}, Gabriel},
  journal={arXiv preprint arXiv:1811.05527},
  year={2018}
}

@inproceedings{altschuler2017near,
  title={Near-linear time approximation algorithms for optimal transport via {Sinkhorn} iteration},
  author={Altschuler, Jason and Niles-Weed, Jonathan and Rigollet, Philippe},
  booktitle={Advances in neural information processing systems 30},
  year={2017}
}

@article{lin2022efficiency,
  title={On the Efficiency of Entropic Regularized Algorithms for Optimal Transport},
  author={Lin, Tianyi and Ho, Nhat and Jordan, Michael I},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={137},
  pages={1--42},
  year={2022}
}

@inproceedings{xie2020differentiable,
  title={Differentiable top-k with optimal transport},
  author={Xie, Yujia and Dai, Hanjun and Chen, Minshuo and Dai, Bo and Zhao, Tuo and Zha, Hongyuan and Wei, Wei and Pfister, Tomas},
  booktitle={Advances in Neural Information Processing Systems 33},
  year={2020}
}

@article{danskin1966theory,
  title={The theory of max-min, with applications},
  author={Danskin, John M},
  journal={SIAM Journal on Applied Mathematics},
  volume={14},
  number={4},
  pages={641--664},
  year={1966},
  publisher={SIAM}
}

@inproceedings{feydy2019interpolating,
  title={Interpolating between optimal transport and {MMD} using {Sinkhorn} divergences},
  author={Feydy, Jean and S{\'e}journ{\'e}, Thibault and Vialard, Fran{\c{c}}ois-Xavier and Amari, Shun-ichi and Trouv{\'e}, Alain and Peyr{\'e}, Gabriel},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={2681--2690},
  year={2019}
}

@inproceedings{corenflos2021differentiable,
  title={Differentiable particle filtering via entropy-regularized optimal transport},
  author={Corenflos, Adrien and Thornton, James and Deligiannidis, George and Doucet, Arnaud},
  booktitle={International Conference on Machine Learning},
  pages={2100--2111},
  year={2021}
}

@article{mena2018learning,
  title={Learning latent permutations with {Gumbel}-{Sinkhorn} networks},
  author={Mena, Gonzalo and Belanger, David and Linderman, Scott and Snoek, Jasper},
  journal={arXiv preprint arXiv:1802.08665},
  year={2018}
}

@inproceedings{liu2015deep,
  title={Deep learning face attributes in the wild},
  author={Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={3730--3738},
  year={2015}
}

@article{cuturi2022optimal,
  title={Optimal transport tools ({OTT}): A {JAX} toolbox for all things {Wasserstein}},
  author={Cuturi, Marco and Meng-Papaxanthos, Laetitia and Tian, Yingtao and Bunne, Charlotte and Davis, Geoff and Teboul, Olivier},
  journal={arXiv preprint arXiv:2201.12324},
  year={2022}
}

@book{higham2002accuracy,
  title={Accuracy and stability of numerical algorithms},
  author={Higham, Nicholas J},
  year={2002},
  publisher={SIAM}
}

@article{fan2002schur,
  title={Schur complements and its applications to symmetric nonnegative and {Z}-matrices},
  author={Fan, Yizheng},
  journal={Linear algebra and its applications},
  volume={353},
  number={1-3},
  pages={289--307},
  year={2002},
  publisher={Elsevier}
}

@book{nocedal2006numerical,
  title={Numerical optimization, 2nd edition},
  author={Nocedal, Jorge and Wright, Stephen J},
  year={2006},
  publisher={Springer}
}

@article{byrd1987global,
  title={Global convergence of a cass of quasi-{Newton} methods on convex problems},
  author={Byrd, Richard H and Nocedal, Jorge and Yuan, Ya-Xiang},
  journal={SIAM Journal on Numerical Analysis},
  volume={24},
  number={5},
  pages={1171--1190},
  year={1987},
  publisher={SIAM}
}

@article{tian2010inequalities,
  title={Inequalities for the minimum eigenvalue of {M}-matrices},
  author={Tian, Gui-Xian and Huang, Ting-Zhu},
  journal={The Electronic Journal of Linear Algebra},
  volume={20},
  pages={291--302},
  year={2010}
}

@inproceedings{dvurechensky2018computational,
  title={Computational optimal transport: Complexity by accelerated gradient descent is better than by {Sinkhorn's} algorithm},
  author={Dvurechensky, Pavel and Gasnikov, Alexander and Kroshnin, Alexey},
  booktitle={International conference on machine learning},
  pages={1367--1376},
  year={2018}
}

@inproceedings{tolstikhin2018wasserstein,
  title={Wasserstein Auto-Encoders},
  author={Tolstikhin, I and Bousquet, O and Gelly, S and Sch{\"o}lkopf, B},
  booktitle={6th International Conference on Learning Representations (ICLR 2018)},
  year={2018}
}

@book{villani2009optimal,
  title={Optimal transport: old and new},
  author={Villani, C{\'e}dric},
  volume={338},
  year={2009},
  publisher={Springer}
}

@article{ChestXray14Dataset,
  title={ChestX-ray14: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases},
  author={Wang, Xiaosong and Peng, Yifan and Lu, Le and Lu, Zhiyong and Bagheri, Mohammadhadi and Summers, Ronald M},
  journal={Proceedings of the IEEE conference on computer vision and pattern recognition},
  year={2017},
  url={https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.html}
}


@article{kloecknerBenoit2012Abfs,
abstract = {We consider the problem of approximating a probability measure defined on a metric space by a measure supported on a finite number of points. More specifically we seek the asymptotic behavior of the minimal Wasserstein distance to an approximation when the number of points goes to infinity. The main result gives an equivalent when the space is a Riemannian manifold and the approximated measure is absolutely continuous and compactly supported.},
author = {Kloeckner, Benoît},
address = {Les Ulis},
copyright = {EDP Sciences, SMAI, 2011},
issn = {1292-8119},
journal = {ESAIM. Control, optimisation and calculus of variations},
keywords = {49Q20 ; 90B85 ; Approximation ; Calculus of variations ; centroidal Voronoi tessellations ; Functional Analysis ; Hypotheses ; location problem ; Mathematical models ; Mathematics ; Measures ; Optimization and Control ; Probability ; quantization ; Random variables ; Studies ; Topological manifolds ; Wasserstein distance},
language = {eng},
number = {2},
pages = {343-359},
publisher = {EDP Sciences},
title = {Approximation by finitely supported measures},
volume = {18},
year = {2012},
}


@book{alma99169167158201081,
abstract = {This monograph presents a rigorous mathematical introduction to optimal transport as a variational problem, its use in modeling various phenomena, and its connections with partial differential equations. Its main goal is to provide the reader with the techniques necessary to understand the current research in optimal transport and the tools which are most useful for its applications. Full proofs are used to illustrate mathematical concepts and each chapter includes a section that discusses applications of optimal transport to various areas, such as economics, finance, potential games, image processing and fluid dynamics. Several topics are covered that have never been previously in books on this subject, such as the Knothe transport, the properties of functionals on measures, the Dacorogna-Moser flow, the formulation through minimal flows with prescribed divergence formulation, the case of the supremal cost, and the most classical numerical methods. Graduate students and researchers in both pure and applied mathematics interested in the problems and applications of optimal transport will find this to be an invaluable resource.},
author = {Santambrogio, Filippo.},
address = {Cham},
booktitle = {Optimal Transport for Applied Mathematicians Calculus of Variations, PDEs, and Modeling},
edition = {1st ed. 2015.},
isbn = {3-319-20828-4},
keywords = {Calculus of variations},
language = {eng},
publisher = {Springer International Publishing},
series = {Progress in Nonlinear Differential Equations and Their Applications, 87},
title = {Optimal Transport for Applied Mathematicians Calculus of Variations, PDEs, and Modeling },
year = {2015},
}


@article{MullerAlfred1997IPMa,
abstract = {We consider probability metrics of the following type: for a class of functions and probability measures P, Q we define A unified study of such integral probability metrics is given. We characterize the maximal class of functions that generates such a metric. Further, we show how some interesting properties of these probability metrics arise directly from conditions on the generating class of functions. The results are illustrated by several examples, including the Kolmogorov metric, the Dudley metric and the stop-loss metric.},
author = {Muller, Alfred},
address = {Cambridge, UK},
copyright = {Copyright © Applied Probability Trust 1997},
issn = {0001-8678},
journal = {Advances in applied probability},
keywords = {Approximation ; Continuous functions ; Convergence ; Functions ; General Applied Probability ; Mathematical functions ; Mathematical models ; Perceptron convergence procedure ; Probability ; Step functions ; Studies ; Sufficient conditions ; Vector spaces ; Weighting functions},
language = {eng},
number = {2},
pages = {429-443},
publisher = {Cambridge University Press},
title = {Integral Probability Metrics and Their Generating Classes of Functions},
volume = {29},
year = {1997},
}


@article{ClaiciSebastian2018WMC,
abstract = {The proliferation of large data sets and Bayesian inference techniques motivates demand for better data sparsification. Coresets provide a principled way of summarizing a large dataset via a smaller one that is guaranteed to match the performance of the full data set on specific problems. Classical coresets, however, neglect the underlying data distribution, which is often continuous. We address this oversight by introducing Wasserstein measure coresets, an extension of coresets which by definition takes into account generalization. Our formulation of the problem, which essentially consists in minimizing the Wasserstein distance, is solvable via stochastic gradient descent. This yields an algorithm which simply requires sample access to the data distribution and is able to handle large data streams in an online manner. We validate our construction for inference and clustering.},
author = {Claici, Sebastian and Genevay, Aude and Solomon, Justin},
address = {Ithaca},
copyright = {2020. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
issn = {2331-8422},
journal = {arXiv (Cornell University)},
keywords = {Cluster analysis ; Clustering ; Construction costs ; Statistical analysis ; Support vector machines ; Transport theory ; Vector quantization},
language = {eng},
publisher = {Cornell University Library, arXiv.org},
title = {Wasserstein Measure Coresets},
year = {2018},
}

@article{LeiJing2020Caco,
author = {Lei, Jing},
issn = {1350-7265},
journal = {Bernoulli : official journal of the Bernoulli Society for Mathematical Statistics and Probability},
language = {eng},
number = {1},
title = {Convergence and concentration of empirical measures under Wasserstein distance in unbounded functional spaces},
volume = {26},
year = {2020},
}

@article{SenerOzan2018ALfC,
abstract = {Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (ie. active learning). Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs in batch setting. Inspired by these limitations, we define the problem of active learning as core-set selection, ie. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points. We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints. As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.},
author = {Sener, Ozan and Savarese, Silvio},
address = {Ithaca},
copyright = {2018. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
issn = {2331-8422},
journal = {arXiv.org},
keywords = {Active learning ; Algorithms ; Artificial neural networks ; Data points ; Image classification ; Machine learning ; Neural networks},
language = {eng},
publisher = {Cornell University Library, arXiv.org},
title = {Active Learning for Convolutional Neural Networks: A Core-Set Approach},
year = {2018},
}

@inproceedings{arjovsky2017wasserstein,
  title={Wasserstein generative adversarial networks},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  booktitle={International conference on machine learning},
  pages={214--223},
  year={2017},
  organization={PMLR}
}

@article{mena2019statistical,
  title={Statistical bounds for entropic optimal transport: sample complexity and the central limit theorem},
  author={Mena, Gonzalo and Niles-Weed, Jonathan},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{genevay2019sample,
  title={Sample complexity of sinkhorn divergences},
  author={Genevay, Aude and Chizat, L{\'e}naic and Bach, Francis and Cuturi, Marco and Peyr{\'e}, Gabriel},
  booktitle={The 22nd international conference on artificial intelligence and statistics},
  pages={1574--1583},
  year={2019},
  organization={PMLR}
}

@article{sinkhorn1964relationship,
  title={A relationship between arbitrary positive matrices and doubly stochastic matrices},
  author={Sinkhorn, Richard},
  journal={The annals of mathematical statistics},
  volume={35},
  number={2},
  pages={876--879},
  year={1964},
  publisher={JSTOR}
}

@article{sinkhorn1967concerning,
  title={Concerning nonnegative matrices and doubly stochastic matrices},
  author={Sinkhorn, Richard and Knopp, Paul},
  journal={Pacific Journal of Mathematics},
  volume={21},
  number={2},
  pages={343--348},
  year={1967},
  publisher={Mathematical Sciences Publishers}
}

@article{peyre2019computational,
  title={Computational optimal transport: With applications to data science},
  author={Peyr{\'e}, Gabriel and Cuturi, Marco and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={11},
  number={5-6},
  pages={355--607},
  year={2019},
  publisher={Now Publishers, Inc.}
}

@inproceedings{cuturi2013sinkhorn,
  title={Sinkhorn distances: Lightspeed computation of optimal transport},
  author={Cuturi, Marco},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@article{schmitzer2019stabilized,
  title={Stabilized sparse scaling algorithms for entropy regularized transport problems},
  author={Schmitzer, Bernhard},
  journal={SIAM Journal on Scientific Computing},
  volume={41},
  number={3},
  pages={A1443--A1481},
  year={2019},
  publisher={SIAM}
}

@inproceedings{genevay2018learning,
  title={Learning generative models with sinkhorn divergences},
  author={Genevay, Aude and Peyr{\'e}, Gabriel and Cuturi, Marco},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1608--1617},
  year={2018},
  organization={PMLR}
}

@article{luise2018differential,
  title={Differential properties of sinkhorn approximation for learning with wasserstein distance},
  author={Luise, Giulia and Rudi, Alessandro and Pontil, Massimiliano and Ciliberto, Carlo},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@inproceedings{cuturi2019differentiable,
  title={Differentiable ranking and sorting using optimal transport},
  author={Cuturi, Marco and Teboul, Olivier and Vert, Jean-Philippe},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{campbell2020solving,
  title={Solving the blind perspective-n-point problem end-to-end with robust differentiable geometric optimization},
  author={Campbell, Dylan and Liu, Liu and Gould, Stephen},
  booktitle={European Conference on Computer Vision},
  pages={244--261},
  year={2020},
  organization={Springer}
}

@article{eisenberger2022scalable,
  title={Scalable Sinkhorn Backpropagation},
  author={Eisenberger, Marvin and Toker, Aysim and Leal-Taix{\'e}, Laura and Bernard, Florian and Cremers, Daniel},
  journal={OpenReview preprint id:uR77O7SL55h},
  year={2022}
}

@inproceedings{cuturi2020supervised,
  title={Supervised quantile normalization for low rank matrix factorization},
  author={Cuturi, Marco and Teboul, Olivier and Niles-Weed, Jonathan and Vert, Jean-Philippe},
  booktitle={International Conference on Machine Learning},
  pages={2269--2279},
  year={2020}
}



@book{rockafellar1970convex,
  title={Convex Analysis},
  author={Rockafellar, R Tyrrell},
  year={1970},
  publisher={Princeton University Press}
}

@book{marshall2011inequalities,
  title={Inequalities: Theory of Majorization and Its Applications, Second Edition},
  author={Marshall, Albert W and Olkin, Ingram and Arnold, Barry C},
  year={2011},
  publisher={Springer Science \& Business Media}
}

@article{liu1989limited,
  title={On the limited memory BFGS method for large scale optimization},
  author={Liu, Dong C and Nocedal, Jorge},
  journal={Mathematical programming},
  volume={45},
  number={1},
  pages={503--528},
  year={1989},
  publisher={Springer}
}

@article{nocedal2002behavior,
  title={On the behavior of the gradient norm in the steepest descent method},
  author={Nocedal, Jorge and Sartenaer, Annick and Zhu, Ciyou},
  journal={Computational Optimization and Applications},
  volume={22},
  number={1},
  pages={5--35},
  year={2002},
  publisher={Springer}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={IEEE}
}

@article{xiao2017fashion,
  title={Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms},
  author={Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  journal={arXiv preprint arXiv:1708.07747},
  year={2017}
}

@article{flamary2021pot,
  title={Pot: Python optimal transport},
  author={Flamary, R{\'e}mi and Courty, Nicolas and Gramfort, Alexandre and Alaya, Mokhtar Z and Boisbunon, Aur{\'e}lie and Chambon, Stanislas and Chapel, Laetitia and Corenflos, Adrien and Fatras, Kilian and Fournier, Nemo and others},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={78},
  pages={1--8},
  year={2021}
}

@incollection{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
